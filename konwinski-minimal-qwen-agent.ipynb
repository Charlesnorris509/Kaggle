{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":84795,"databundleVersionId":10821240,"sourceType":"competition"},{"sourceId":3623845,"sourceType":"datasetVersion","datasetId":2171258},{"sourceId":162822,"sourceType":"modelInstanceVersion","modelInstanceId":138458,"modelId":161088},{"sourceId":169042,"sourceType":"modelInstanceVersion","modelInstanceId":143819,"modelId":166403}],"dockerImageVersionId":30823,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"A minimal soltion to the problem by creating solver agents","metadata":{}},{"cell_type":"code","source":"!pip install accelerate\n!pip install einops\n#!pip install - U bitsandbytes\n!pip install transformers_stream_generator==0.0.4\n#!pip install -U --no-index --find-links=/kaggle/input/vllm-whl -U vllm\n#!pip install -U --upgrade /kaggle/input/vllm-t4-fix/grpcio-1.62.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n#!pip install -U --upgrade /kaggle/input/vllm-t4-fix/ray-2.11.0-cp310-cp310-manylinux2014_x86_64.whl\n#https://www.kaggle.com/code/tranhoangquan/aimo-ss2-quan/notebook\n!unzip -nq ../input/konwinski-prize/data.a_zip","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport torch\nfrom typing import List\nfrom transformers import AutoModelForCausalLM, AutoTokenizer,AutoModel\nfrom transformers import RobertaTokenizer, RobertaModel\nimport io\nimport os\nimport shutil\nfrom pathlib import Path\nimport fnmatch\nfrom git import Repo\nimport subprocess\nimport gc\nimport kaggle_evaluation.konwinski_prize_inference_server\nimport numpy as np\nfrom difflib import SequenceMatcher\nfrom tqdm import tqdm\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom datetime import datetime\ntrain_data = pd.read_parquet(\"/kaggle/working/data/data.parquet\")\ntrain_data['instance_id'] = \"repo__\" + train_data['instance_id']\ntrain_data","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_data.info()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_data.sample()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model_path = \"/kaggle/input/codebert-base/codebert-base\"\ntokenizer = RobertaTokenizer.from_pretrained(\n    model_path,\n    local_files_only=True,\n    vocab_file=os.path.join(model_path, \"vocab.json\"),\n    merges_file=os.path.join(model_path, \"merges.txt\")\n)\n\nmodel = RobertaModel.from_pretrained(\n    model_path,\n    local_files_only=True,\n    config=os.path.join(model_path, \"config.json\"),\n    state_dict=torch.load(os.path.join(model_path, \"pytorch_model.bin\"))\n)\n\ndef calculate_semantic_similarity(pred: str, truth: str, model, tokenizer) -> float:\n    inputs = tokenizer([pred, truth], \n                      padding=True, \n                      truncation=True, \n                      max_length=512, \n                      return_tensors=\"pt\").to(model.device)\n    \n    with torch.no_grad():\n        outputs = model(**inputs)\n    \n    attention_mask = inputs.attention_mask.unsqueeze(-1)\n    embeddings = (outputs.last_hidden_state * attention_mask).sum(dim=1) / attention_mask.sum(dim=1).clamp(min=1e-9)\n    \n    return cosine_similarity(embeddings[0].cpu().numpy().reshape(1, -1), \n                            embeddings[1].cpu().numpy().reshape(1, -1))[0][0]\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def find_most_related_file(text_input, repo_dir, top_n=1, max_file_size=4000):\n    code_files = []\n    for root, _, files in os.walk(repo_dir):\n        for file in files:\n            if file.endswith(('.py', '.java', '.cpp', '.js', '.ts', '.c', '.h')):\n                filepath = os.path.join(root, file)\n                try:\n                    # Check file size\n                    if os.path.getsize(filepath) > max_file_size * 1024:  # Convert KB to bytes\n                        continue\n                    \n                    with open(filepath, 'r', encoding='utf-8', errors='ignore') as f:\n                        content = f.read()\n                    code_files.append((filepath, content))\n                except Exception as e:\n                    print(f\"Error reading file {filepath}: {e}\")\n    \n    if not code_files:\n        print(\"No code files found in the repository directory.\")\n        return []\n    code_df = pd.DataFrame(code_files, columns=['file_path', 'file_content'])\n    \n    file_contents = code_df['file_content'].values.astype(str).tolist()\n    all_texts = [str(text_input)] + file_contents  # Ensure text_input is also a string\n    \n    vectorizer = TfidfVectorizer(stop_words='english')\n    tfidf_matrix = vectorizer.fit_transform(all_texts)\n    \n    text_vector = tfidf_matrix[0]\n    file_vectors = tfidf_matrix[1:]\n    similarity_scores = cosine_similarity(text_vector, file_vectors).flatten()\n    \n    top_indices = similarity_scores.argsort()[-top_n:][::-1]\n    top_files = [code_df.iloc[i]['file_path'] for i in top_indices]\n    return top_files\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Function to summarize the repo\ndef get_repo_structure_fallback(repo_path: str, max_depth: int = 3) -> str:\n    \"\"\"Recursive directory structure listing\"\"\"\n    structure = []\n    \n    def _walk(path: Path, current_depth: int):\n        if current_depth > max_depth:\n            return\n            \n        # Add directory entry\n        dir_entry = f\"{'  ' * (current_depth-1)}ðŸ“ {path.name}/\"\n        structure.append(dir_entry)\n        \n        # List files first\n        files = sorted([f for f in path.iterdir() if f.is_file()])\n        for f in files[:5]:  # Limit files per directory\n            structure.append(f\"{'  ' * current_depth}ðŸ“„ {f.name}\")\n            \n        # Recursively list directories\n        dirs = sorted([d for d in path.iterdir() if d.is_dir() \n                      and d.name not in ['.git', '__pycache__', 'node_modules']])\n        for d in dirs[:5]:  # Limit subdirectories\n            _walk(d, current_depth + 1)\n    \n    try:\n        _walk(Path(repo_path), 1)\n        return \"\\n\".join(structure)[:2000]  # Truncate long outputs\n    except Exception as e:\n        return f\"Error generating structure: {str(e)}\"\ndef get_git_aware_structure(repo_path: str) -> str:\n    \"\"\"Get structure with git history insights\"\"\"\n    structure = []\n    repo = Repo(repo_path)\n    \n    # Get recent authors\n    contributors = set()\n    for commit in repo.iter_commits('HEAD', max_count=10):\n        contributors.add(commit.author.name)\n    \n    # Get modified files\n    modified = [item.a_path for item in repo.index.diff(None)]\n    \n    # Build structure\n    structure.append(f\"Recent contributors: {', '.join(contributors)[:100]}\")\n    structure.append(\"Recent modified files:\")\n    structure.extend(modified[:10])\n    \n    return \"\\n\".join(structure)\n    \ndef identify_important_files(repo_path: str) -> str:\n    \"\"\"Identify likely important files\"\"\"\n    priority_files = []\n    \n    # Common important file patterns\n    important_patterns = [\n        'requirements.txt', 'setup.py', 'package.json',\n        'Dockerfile', 'Makefile', '*.md',\n        'src/', 'lib/', 'main.py', 'app.py'\n    ]\n    \n    for root, _, files in os.walk(repo_path):\n        for f in files:\n            path = Path(root) / f\n            rel_path = path.relative_to(repo_path)\n            \n            # Check against known patterns\n            if any(fnmatch.fnmatch(str(rel_path), pat) for pat in important_patterns):\n                priority_files.append(f\"* {rel_path}\")\n                \n            # Check file size\n            if path.stat().st_size > 100000:  # >100KB\n                priority_files.append(rel_path)\n    \n    return priority_files[:20]\ndef analyze_repo_structure(repo_path: str) -> str:\n    \"\"\"Combine multiple structure analysis methods\"\"\"\n    parts = []\n    \n    # Basic directory structure\n    parts.append(\"## Directory Structure ##\")\n    parts.append(get_repo_structure_fallback(repo_path))\n    \n    # Important files analysis\n    parts.append(\"\\n## Key Files ##\")\n    parts.append(identify_important_files(repo_path))\n    \n    # Git insights\n    try:\n        parts.append(\"\\n## Development Insights ##\")\n        parts.append(get_git_aware_structure(repo_path))\n    except:\n        pass\n    \n    return parts\n#analyze_repo_structure(repo_path)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def cosine_similarity_reviews(text1, text2):\n    # Combine the two reviews into a list\n    reviews = [text1, text2]\n    \n    # Step 1: Compute TF-IDF embeddings for both reviews\n    vectorizer = TfidfVectorizer(stop_words='english')\n    tfidf_matrix = vectorizer.fit_transform(reviews)\n    \n    # Step 2: Calculate cosine similarity\n    similarity_score = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])[0][0]\n    \n    return similarity_score","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Example Usage\ntext_input = train_data['problem_statement'].values[4]\nprint(\"issue: \", text_input[:200])\nrepo_dir = \"/kaggle/working/data/repos\"\nprint(datetime.now().strftime(\"Hour: %H, Minute: %M, Second: %S\"))\ntop_related_file = find_most_related_file(text_input, repo_dir, top_n=1)\nprint(datetime.now().strftime(\"Hour: %H, Minute: %M, Second: %S\"))\nfor file_path in top_related_file:\n        print(f\"File: {file_path}\\n\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"torch.cuda.empty_cache()\ngc.collect()\n#os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:512,garbage_collection_threshold:0.8,expandable_segments:True\"\n\nclass GithubIssueSolver:\n    def __init__(self):\n        model_name = \"/kaggle/input/qwen2.5-coder/transformers/7b-instruct/1\"#\n        \n        # More conservative memory settings\n        #max_memory = {0: \"12GiB\", \"cpu\": \"12GiB\"}\n        \n        self.model = AutoModelForCausalLM.from_pretrained(\n            model_name,\n            torch_dtype=torch.float16,\n            device_map=\"auto\",\n            #max_memory=max_memory,\n            low_cpu_mem_usage=True,\n            offload_folder=\"offload\",\n        )\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n        self.max_context_length = 16384  # Reduced from 4096\n        self.max_file_size = 2000  # Reduced from 1000\n    def _create_prompt(self, problem_statement: str, context: str, repo_structure: str) -> str:\n        return f\"\"\"**Task**: Fix the GitHub issue by generating a correct git diff patch. Use strict step-by-step reasoning.\n    \n                **Format Requirements**:\n                1. Output MUST start with 'diff --git'\n                2. Understand where to modify from relevant files (max 3 files)\n                3. Include precise line numbers\n                4. Never write code comments unless present in original\n                5. Include only necessary changes\n                \n                **Problem Analysis Framework**:\n                1. Root Cause Identification:\n                   - Identify specific components causing the issue\n                   - Analyze error patterns from problem description\n                \n                2. Code Context Mapping:\n                   - Match issue components to relevant code sections\n                   - File structure: {repo_structure}\n                   - Relevant code snippets: {context}\n                \n                3. Change Validation:\n                   - Cross-verify each change against problem statement\n                   - Ensure no unrelated code modifications\n                \n                **Example of Good Patch**:\n                diff --git a/file.py b/file.py\n                --- a/file.py\n                +++ b/file.py\n                @@ -12,7 +12,7 @@\n                     try:\n                -        result = process(data)\n                +        result = process(data, timeout=30)\n                     except TimeoutError:\n                -        logger.warning(\"Timeout occurred\")\n                +        logger.error(\"Timeout (30s) exceeded\", exc_info=True)\n                \n                **Current Issue**:\n                {problem_statement}\n                \n                **Step-by-Step Process**:\n                1. Identify key components needing modification\n                2. Locate exact lines in relevant files\n                3. Make minimal changes to fix issue\n                4. Verify against all mentioned edge cases\n                \n                **Output Instructions**:\n                - Start immediately with diff patch\n                - Use exact file paths from repository\n                - Include confidence score (0-100) as last line\n                - If uncertain, output \"SKIP\" with reason\n                \n                **Begin Fix**:\n                \"\"\"\n    def analyze_issue(self, problem_statement: str, repo_path: str) -> str:\n        try:\n            torch.cuda.empty_cache()\n            gc.collect()\n            \n            relevant_files = self._find_relevant_files(repo_path, problem_statement)\n            if not relevant_files:\n                print(\"No relevant files found\")\n                return None\n                \n            context = self._read_file_contents(repo_path, relevant_files)\n            repo_structure = analyze_repo_structure(repo_path)\n            prompt= self._create_prompt(problem_statement, context, repo_structure)\n            #modified CoT prompt from: https://arxiv.org/pdf/2501.05040\n            messages = [\n                {\"role\": \"system\", \"content\": \"\"\"\"\n                                    You are an expert software engineer and code reviewer specializing in resolving real-world GitHub issues by creating precise diff patches. Your role is to analyze the issue description and the most relevant code segments from the repository (pre-identified using cosine similarity) to propose effective and accurate code modifications.\n                                    \n                                    In this task, you will:\n                                    \n                                    1. **Understand the Issue:** Carefully analyze the provided GitHub issue description to identify the root cause and the functional or structural problem in the code.\n                                    2. **Analyze Relevant Code:** Examine the provided code snippets or files identified as most relevant to the issue. Use your expertise to determine the exact areas requiring modification.\n                                    3. **Generate a Diff Patch:** Create a detailed and well-structured diff patch that resolves the issue while maintaining the integrity and functionality of the codebase.\n                                    4. **Provide Reasoning:** Accompany your patch with a clear, step-by-step explanation of your reasoning process, detailing how the proposed changes address the issue effectively.\n                                    \n                                    ### Guidelines:\n                                    - **Independent Reasoning:** Your analysis and diff patch should be based solely on the issue description and the provided code snippets. Avoid referencing external solutions or implying prior knowledge of oracle modifications.\n                                    - **Clarity and Precision:** Ensure that your diff patch is syntactically correct, adheres to best coding practices, and is easy to apply.\n                                    - **Evidence-Based Reasoning:** Clearly justify your changes, linking them to specific parts of the issue description and code. Highlight how the modifications resolve the issue and improve the codebase.\n                                    \n                                    This task focuses on accurately resolving GitHub issues through diff patches while maintaining high standards of clarity, precision, and logical consistency.\n                                                \"\"\"},\n                {\"role\": \"user\", \"content\": prompt}\n            ]\n            \n            text = self.tokenizer.apply_chat_template(\n                messages,\n                tokenize=False,\n                add_generation_prompt=True\n            )\n            \n            # Split processing into smaller chunks\n            inputs = self.tokenizer(\n                text, \n                return_tensors=\"pt\", \n                truncation=True,\n                max_length=self.max_context_length\n            ).to(self.model.device)\n            \n            with torch.inference_mode():\n                try:\n                    generated_ids = self.model.generate(\n                        input_ids=inputs['input_ids'],\n                        max_new_tokens=16384,  # Reduced from 1024\n                        temperature=0.8,#from 0.7\n                        do_sample=True,\n                        pad_token_id=self.tokenizer.pad_token_id,\n                        num_return_sequences=1,\n                    )\n                    \n                    response = self.tokenizer.decode(\n                        generated_ids[0, inputs['input_ids'].shape[1]:],\n                        skip_special_tokens=True\n                    )\n                finally:\n                    del inputs\n                    torch.cuda.empty_cache()\n                    gc.collect()\n            \n            if \"diff --git\" in response:\n                diff_start = response.find(\"diff --git\")\n                return response[diff_start:]\n            return None\n            \n        except Exception as e:\n            print(f\"Error generating solution: {str(e)}\")\n            return None\n        finally:\n            torch.cuda.empty_cache()\n            gc.collect()\n\n    def _find_relevant_files(self, repo_path: str, problem_statement: str) -> list:\n        relevant_files = []\n        #relevant_files = find_most_related_file(problem_statement, repo_path, top_n=2)\n        #print(\"type(repo_path): \", type(repo_path))\n        #print(\"repo_path: \", repo_path)\n        #print(\"relevant_files: \", relevant_files[:4])\n        try:\n            keywords = set(problem_statement.lower().split())\n            #print(\"keywords: \", keywords[:25])\n            for root, _, files in os.walk(repo_path):\n                if len(relevant_files) >= 2:\n                    break\n                    \n                for file in files:\n                    if file.endswith(('.py', '.java', '.cpp', '.h', '.c', '.js', '.ts')):\n                        file_path = os.path.join(root, file)\n                        try:\n                            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n                                content = f.read(self.max_file_size * 50)  # Read with size limit\n                                \n                            if any(word in content.lower() for word in keywords):\n                                relevant_files.append(os.path.relpath(file_path, repo_path))\n                                if len(relevant_files) >= 2:\n                                    break\n                        except Exception:\n                            continue\n                                \n        except Exception as e:\n            print(f\"Error finding relevant files: {str(e)}\")\n      \n        print(\"num of relevant files\",  len(relevant_files))\n        print(\"relevant files\",  relevant_files)\n        return relevant_files\n\n    def _read_file_contents(self, repo_path: str, files: list) -> str:\n        contents = []\n        total_lines = 0\n        \n        for file in files:\n            if total_lines >= self.max_file_size:\n                break\n                \n            try:\n                with open(os.path.join(repo_path, file), 'r', encoding='utf-8', errors='ignore') as f:\n                    lines = []\n                    for i, line in enumerate(f):\n                        if i >= self.max_file_size // len(files):\n                            break\n                        lines.append(line)\n                    contents.append(f\"File: {file}\\n{''.join(lines)}\")\n                    total_lines += len(lines)\n            except Exception:\n                continue\n        print(\"file content: \", \"\\n\".join(contents)[:200])\n        return \"\\n\".join(contents)\n\n# Global solver instance\nsolver = None","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if solver is None:\n    solver = GithubIssueSolver()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data_t = train_data.sample()\ndata_t","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"repo_name = data_t['instance_id'].values[0]#.replace('/', '_')\nbase_repo_path = \"/kaggle/working/data/repos/\"\n#repo_path = base_repo_path + repo_name\nrepo_path = os.path.join(base_repo_path, repo_name)\nif not os.path.exists(repo_path):\n    raise ValueError(f\"Repository path does not exist: {repo_path}\")\nprint(\"repo_path: \",repo_path)\n# Prepare inputs\nproblem_statement = data_t['problem_statement']\nprint(\"problem_statement: \",problem_statement[:25])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(datetime.now().strftime(\"Hour: %H, Minute: %M, Second: %S\")) \nresponse = solver.analyze_issue(\n    problem_statement.values[0],\n    repo_path)\nprint(datetime.now().strftime(\"Hour: %H, Minute: %M, Second: %S\")) ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from IPython.display import display, Markdown, Latex\ndisplay(Markdown(response))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from unidiff import PatchSet\ndef is_valid_diff(patch):\n    try:\n        PatchSet(patch)\n        return True\n    except Exception:\n        return False\nis_valid_diff(response)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"display(Markdown(data_t['patch'].values[0]))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#cosine_sim = calculate_semantic_similarity(data_t['patch'].values[0], response, model, tokenizer)\n#print(\"Cosine Similarity: \", cosine_sim)#prev: Cosine Similarity:  0.11839781672838229","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"instance_count = None\ndef get_number_of_instances(num_instances: int) -> None:\n    global instance_count\n    instance_count = num_instances\n\ndef predict(\n    problem_statement: str, \n    repo_archive: io.BytesIO, \n    pip_packages_archive: io.BytesIO, \n    env_setup_cmds_templates: list[str]\n) -> str:\n    global solver\n    \n    # Define repo_path at the start\n    repo_path = os.path.join(os.getcwd(), 'repo')\n    \n    try:\n        if solver is None:\n            solver = GithubIssueSolver()\n        \n        # Clean up any existing repo directory\n        if os.path.exists(repo_path):\n            shutil.rmtree(repo_path)\n        \n        # Create archive file and extract\n        archive_path = os.path.join(os.getcwd(), 'repo_archive.tar')\n        with open(archive_path, 'wb') as f:\n            f.write(repo_archive.read())\n            \n        os.makedirs(repo_path, exist_ok=True)\n        shutil.unpack_archive(archive_path, repo_path)\n        os.remove(archive_path)\n        sol = solver.analyze_issue(problem_statement, repo_path)\n        print(\"sol: \", sol)\n        # Process the issue\n        return sol\n        \n    except Exception as e:\n        print(f\"Error in predict function: {str(e)}\")\n        return None\n    finally:\n        # Clean up\n        if os.path.exists(repo_path):\n            shutil.rmtree(repo_path)\n        #torch.cuda.empty_cache()\n        #gc.collect()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"inference_server = kaggle_evaluation.konwinski_prize_inference_server.KPrizeInferenceServer(\n    get_number_of_instances,   \n    predict\n)\nif os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n    inference_server.serve()\nelse:\n    inference_server.run_local_gateway(\n        data_paths=(\n            '/kaggle/input/konwinski-prize/',\n            '/kaggle/tmp/konwinski-prize/',\n        ),\n        use_concurrency=True,\n    )","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}